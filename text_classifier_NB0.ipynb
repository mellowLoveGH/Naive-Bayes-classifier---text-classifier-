{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classifier_NB0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO-RfDbY7Gky"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download()\r\n",
        "# here to download movie_reviews and stopwords packages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8_1XHTk-Q2X"
      },
      "source": [
        "from nltk.corpus import movie_reviews as mr\r\n",
        "print(mr.readme())\r\n",
        "\r\n",
        "from nltk.corpus import stopwords\r\n",
        "#print(stopwords.words('english'))\r\n",
        "sw = stopwords.words('english');\r\n",
        "print(len(sw))\r\n",
        "\r\n",
        "\r\n",
        "# here in the stopwords list 'sw'\r\n",
        "# it should remove some words from the list\r\n",
        "# such as: 'no','nor','not','too',\r\n",
        "sw.remove('no')\r\n",
        "sw.remove('nor')\r\n",
        "sw.remove('not')\r\n",
        "sw.remove('too')\r\n",
        "# \"don't\",\"aren't\", \"couldn't\", 'didn', \"didn't\", \r\n",
        "# \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\",\r\n",
        "#  \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\",\r\n",
        "#  \"won't\", \"wouldn't\"\r\n",
        "sw1 = []\r\n",
        "for v in sw:\r\n",
        "    if not v.endswith(\"n't\"):\r\n",
        "        sw1.append(v)\r\n",
        "    #else:\r\n",
        "        #print(v)\r\n",
        "sw1.remove('ain')\r\n",
        "sw1.remove('aren')\r\n",
        "sw1.remove('couldn')\r\n",
        "sw1.remove('didn')\r\n",
        "sw1.remove('doesn')\r\n",
        "sw1.remove('hadn')\r\n",
        "sw1.remove('haven')\r\n",
        "sw1.remove('isn')\r\n",
        "sw1.remove('mightn')\r\n",
        "sw1.remove('hasn')\r\n",
        "sw1.remove('mustn')\r\n",
        "sw1.remove('needn')\r\n",
        "sw1.remove('shan')\r\n",
        "sw1.remove('wasn')\r\n",
        "sw1.remove('weren')\r\n",
        "sw1.remove('wouldn')\r\n",
        "sw = sw1\r\n",
        "sw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEh9y8sz-xgV",
        "outputId": "9682c467-f1b6-4a13-90b0-b2213ab6a0a5"
      },
      "source": [
        "# divide the movie_reviews into 2 categories: positive and negative\r\n",
        "positive = mr.fileids('pos')\r\n",
        "negative = mr.fileids('neg')\r\n",
        "\r\n",
        "# form a new movie_reviews as a dictionary by positive and negative\r\n",
        "new_mr = dict(pos = positive, neg = negative)\r\n",
        "\r\n",
        "# the length of positive and negative lists are both 1000\r\n",
        "posLen = len(new_mr['pos'])\r\n",
        "print(posLen)\r\n",
        "negLen = len(new_mr['neg'])\r\n",
        "print(negLen)\r\n",
        "\r\n",
        "# get by text string\r\n",
        "#print(mr.raw(new_mr['pos'][0]))\r\n",
        "print(len(mr.raw(new_mr['pos'][0])))\r\n",
        "#print(mr.raw(new_mr['neg'][0]))\r\n",
        "print(len(mr.raw(new_mr['neg'][0])))\r\n",
        "# get by words\r\n",
        "#print(mr.words(new_mr['pos'][0])\r\n",
        "print(len(mr.words(new_mr['pos'][0])))\r\n",
        "#print(mr.words(new_mr['pos'][0])\r\n",
        "print(len(mr.words(new_mr['neg'][0])))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "1000\n",
            "4227\n",
            "4043\n",
            "862\n",
            "879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKH_vSmjA0KV"
      },
      "source": [
        "# the input is the whole text string\r\n",
        "# improvement: remove stopwords and other useless words, punctuations\r\n",
        "def unigram_features (words):\r\n",
        "    \"\"\"\r\n",
        "    This is the simplest possible feature representation of a document.\r\n",
        "\r\n",
        "    Each word is a feature.\r\n",
        "    \"\"\"\r\n",
        "    return dict((word, True) for word in words)\r\n",
        "\r\n",
        "# input a word, \r\n",
        "# determine whether it is a stopword or not\r\n",
        "# remove all punctuations from the word\r\n",
        "def pre_process(word):\r\n",
        "  # remove punctuations except apostrophe\r\n",
        "  w = \"\"\r\n",
        "  hasAlphabet = False\r\n",
        "  for i in word:\r\n",
        "    if i >= 'a' and i <= 'z':\r\n",
        "      w = w + i\r\n",
        "      hasAlphabet = True\r\n",
        "    elif i >= 'A' and i <= 'Z':\r\n",
        "      w = w + i\r\n",
        "      hasAlphabet = True\r\n",
        "    elif i == '\\'': # apostrophe\r\n",
        "      w = w + i\r\n",
        "  #print(w)\r\n",
        "  # check whether it is stopword\r\n",
        "  if hasAlphabet and w not in sw:\r\n",
        "    return w\r\n",
        "  return ''\r\n",
        "\r\n",
        "word = \",,wouldn't..\"\r\n",
        "pre_process(word);\r\n",
        "\r\n",
        "\r\n",
        "# the input parameter is a list consisting of words\r\n",
        "# and return a dictionary which includes useful words after pre-processing\r\n",
        "def feature_selection(words):\r\n",
        "  wd = {}\r\n",
        "  for word in words:\r\n",
        "    word = pre_process(word)\r\n",
        "    if len(word) > 0 and word not in wd:\r\n",
        "      wd[word] = True\r\n",
        "  return wd\r\n",
        "\r\n",
        "d1 = feature_selection(mr.words(new_mr['pos'][0]))\r\n",
        "print(len(d1))\r\n",
        "print(d1)\r\n",
        "d2 = unigram_features(mr.words(new_mr['pos'][0]))\r\n",
        "print(len(d2))\r\n",
        "print(d2)\r\n",
        "\r\n",
        "d1 = feature_selection(mr.words(new_mr['neg'][0]))\r\n",
        "print(len(d1))\r\n",
        "print(d1)\r\n",
        "d2 = unigram_features(mr.words(new_mr['neg'][0]))\r\n",
        "print(len(d2))\r\n",
        "print(d2)\r\n",
        "\r\n",
        "# extract a file that is a move review from the nltk.corpus's movie_reviews\r\n",
        "def extract_features (corpus, file_ids, cls, feature_extractor=unigram_features):\r\n",
        "    \"\"\"\r\n",
        "    Turn a set of files all belonging to one class into a list\r\n",
        "    of (feature dictionary, cls) pairs, to be used in testing or training\r\n",
        "    a classifier.\r\n",
        "    \"\"\"\r\n",
        "    return [(feature_extractor(corpus.words(i)), cls) for i in file_ids]\r\n",
        "\r\n",
        "# collect several/some/many dict-type positive or negative movie reviews into a list\r\n",
        "# for training or testing\r\n",
        "# the returned list is composed of tuple\r\n",
        "# every tuple is the ict-type positive or negative movie reviews + clf\r\n",
        "# corpus, should be the 'mr'\r\n",
        "# files, should be the 'new_mr'\r\n",
        "# clf, is the classification: pos or neg\r\n",
        "def collect_features(corpus, files, startID, endID, clf):\r\n",
        "  col = []\r\n",
        "  i = startID\r\n",
        "  while i < endID:\r\n",
        "    dic = feature_selection(corpus.words(files[clf][i]))\r\n",
        "    tup = (dic, clf)\r\n",
        "    col.append(tup)\r\n",
        "    i = i + 1\r\n",
        "  return col\r\n",
        "\r\n",
        "# return every word from the given removie reviews\r\n",
        "# file_ids, means the the range, to get all movie reviews in this range\r\n",
        "def get_words_from_corpus (corpus, file_ids):\r\n",
        "    for file_id in file_ids:\r\n",
        "        words = corpus.words(file_id)\r\n",
        "        for word in words:\r\n",
        "            yield word\r\n",
        "\r\n",
        "\r\n",
        "tmp = extract_features(mr, new_mr['neg'][:100], 'neg', feature_extractor=feature_selection)\r\n",
        "print(len(tmp))\r\n",
        "print(tmp[0])\r\n",
        "print(len(tmp[0]))\r\n",
        "print()\r\n",
        "\r\n",
        "# for example, choose the first 100 files to be the training set\r\n",
        "tmp = collect_features(mr, new_mr, 0, 100, 'neg')\r\n",
        "print(len(tmp))\r\n",
        "print(tmp[0])\r\n",
        "print(len(tmp[0]))\r\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqC6_rSpBxD-",
        "outputId": "37701f38-c7ba-43e2-8f0c-0e135d03d41d"
      },
      "source": [
        "# selection the 3000 most important words.\r\n",
        "# find the 3000 most important words by frequency\r\n",
        "# top, is how many words in the list to be returned\r\n",
        "def importantWords(corpus, file, top):\r\n",
        "  iw = []\r\n",
        "  words_dic = {}\r\n",
        "  pLen = len(file['pos'])\r\n",
        "  i = 0\r\n",
        "  while i < pLen:\r\n",
        "    movie_review = corpus.words(file['pos'][i])\r\n",
        "    for w in movie_review:\r\n",
        "      w = pre_process(w)\r\n",
        "      if w not in words_dic:\r\n",
        "        words_dic[w] = 1\r\n",
        "      else:\r\n",
        "        words_dic[w] = words_dic[w] + 1\r\n",
        "    i = i + 1\r\n",
        "  nLen = len(file['neg'])\r\n",
        "  i = 0\r\n",
        "  while i < nLen:\r\n",
        "    movie_review = corpus.words(file['neg'][i])\r\n",
        "    for w in movie_review:\r\n",
        "      w = pre_process(w)\r\n",
        "      if len(w) > 0:\r\n",
        "        if w not in words_dic:\r\n",
        "          words_dic[w] = 1\r\n",
        "        else:\r\n",
        "          words_dic[w] = words_dic[w] + 1\r\n",
        "      else:\r\n",
        "        continue\r\n",
        "    i = i + 1\r\n",
        "  words_dic.pop('') # remove space\r\n",
        "  sorted_words_dic = sorted(words_dic.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)\r\n",
        "  # add the top words to the list to be returned\r\n",
        "  i = 0\r\n",
        "  while i < top:\r\n",
        "    iw.append(sorted_words_dic[i])\r\n",
        "    i = i + 1\r\n",
        "  return iw\r\n",
        "\r\n",
        "top = 10\r\n",
        "importantWords(mr, new_mr, top)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('film', 9519),\n",
              " ('one', 5853),\n",
              " ('movie', 5774),\n",
              " ('not', 5583),\n",
              " ('like', 3690),\n",
              " ('even', 2565),\n",
              " ('no', 2473),\n",
              " ('time', 2411),\n",
              " ('good', 2411),\n",
              " ('story', 2170)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oakWCNUNAdk3"
      },
      "source": [
        "# Use a Naive Bayes Classifier\r\n",
        "#from nltk.classify import NaiveBayesClassifier\r\n",
        "#data = dict(pos = mr.fileids('pos'),neg = mr.fileids('neg'))\r\n",
        "\r\n",
        "# Use 90% of the data for training\r\n",
        "test_start_index = 900\r\n",
        "neg_training = extract_features(mr, data['neg'][:test_start_index], 'neg', feature_extractor=unigram_features)\r\n",
        "#print(neg_training[0])\r\n",
        "# Use 10% for testing the classifier on unseen data.\r\n",
        "neg_test = extract_features(mr, data['neg'][test_start_index:], 'neg', feature_extractor=unigram_features)\r\n",
        "\r\n",
        "pos_training = extract_features(mr, data['pos'][:test_start_index],'pos', feature_extractor=unigram_features)\r\n",
        "pos_test = extract_features(mr, data['pos'][test_start_index:],'pos', feature_extractor=unigram_features)\r\n",
        "\r\n",
        "train_set = pos_training + neg_training\r\n",
        "test_set = pos_test + neg_test\r\n",
        "\r\n",
        "print(len(neg_training))\r\n",
        "print(len(neg_test))\r\n",
        "print(len(pos_training))\r\n",
        "print(len(pos_test))\r\n",
        "print(len(train_set))\r\n",
        "print(len(test_set))\r\n",
        "print()\r\n",
        "\r\n",
        "# \r\n",
        "startID = 0\r\n",
        "testID = 900\r\n",
        "pos_training = collect_features(mr, new_mr, startID, testID, 'pos')\r\n",
        "pos_test = collect_features(mr, new_mr, testID, posLen, 'pos')\r\n",
        "\r\n",
        "neg_training = collect_features(mr, new_mr, startID, testID, 'neg')\r\n",
        "neg_test = collect_features(mr, new_mr, testID, negLen, 'neg')\r\n",
        "\r\n",
        "# whole sets\r\n",
        "train_set = pos_training + neg_training\r\n",
        "test_set = pos_test + neg_test\r\n",
        "\r\n",
        "print(len(neg_training))\r\n",
        "print(len(neg_test))\r\n",
        "print(len(pos_training))\r\n",
        "print(len(pos_test))\r\n",
        "print(len(train_set))\r\n",
        "print(len(test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ofG5BF5BZy1"
      },
      "source": [
        "# text classification\r\n",
        "# Use a Naive Bayes Classifier\r\n",
        "from nltk.classify import NaiveBayesClassifier\r\n",
        "# Train a classifier on our training data.\r\n",
        "classifier = NaiveBayesClassifier.train(train_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdZ9gkaJBuun"
      },
      "source": [
        "def get_review_text (clf,file_id,start=0,end=None):\r\n",
        "    words = list(mr.words(data[clf][file_id]))\r\n",
        "    return ' '.join(words[start:end])\r\n",
        "\r\n",
        "def get_mr_text(corpus, files, clf, fileID, startID=0, endID=None):\r\n",
        "  words = list(corpus.words(files[clf][fileID]))\r\n",
        "  return ' '.join(words[startID:endID])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofouU9hCCEgE"
      },
      "source": [
        "#print(get_mr_text(mr, new_mr, 'pos', 900) )\r\n",
        "#print(get_mr_text(mr, new_mr, 'neg', 900) )\r\n",
        "pre_pos_right = 0\r\n",
        "pre_neg_right = 0\r\n",
        "pre_pos_wrong = 0\r\n",
        "pre_neg_wrong = 0\r\n",
        "i = 0\r\n",
        "while i < 100:\r\n",
        "  predicted_label0 = classifier.classify(pos_test[i][0])\r\n",
        "  #print(pos_test[i][0])\r\n",
        "  if predicted_label0 == 'pos':\r\n",
        "    pre_pos_right = pre_pos_right + 1\r\n",
        "  if predicted_label0 == 'neg':\r\n",
        "    pre_pos_wrong = pre_pos_wrong + 1\r\n",
        "  print('Predicted: %s \\t Actual: pos ' %predicted_label0)\r\n",
        "  predicted_label1 = classifier.classify(neg_test[i][0])\r\n",
        "  #print(neg_test[i][0])\r\n",
        "  if predicted_label1 == 'neg':\r\n",
        "    pre_neg_right = pre_neg_right + 1\r\n",
        "  if predicted_label1 == 'pos':\r\n",
        "    pre_neg_wrong = pre_neg_wrong + 1\r\n",
        "  print('Predicted: %s \\t Actual: neg ' %predicted_label1)\r\n",
        "  i = i + 1\r\n",
        "\r\n",
        "print(pre_pos_right)\r\n",
        "print(pre_neg_right)\r\n",
        "print(pre_pos_wrong)\r\n",
        "print(pre_neg_wrong)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v6_p2p0DZ85"
      },
      "source": [
        "# new testing examples excluded from the move_reviews set\r\n",
        "prediction = classifier.classify(unigram_features('Inception is the best movie ever'.split()))\r\n",
        "print(prediction)\r\n",
        "prediction = classifier.classify(unigram_features(\"I don't know how anyone could sit through Inception\".split()))\r\n",
        "print(prediction)\r\n",
        "\r\n",
        "classifier.show_most_informative_features()\r\n",
        "\r\n",
        "# \r\n",
        "prediction = classifier.classify(feature_selection('Inception is the best movie ever'.split()))\r\n",
        "print(prediction)\r\n",
        "prediction = classifier.classify(feature_selection(\"I don't know how anyone could sit through Inception\".split()))\r\n",
        "print(prediction)\r\n",
        "\r\n",
        "classifier.show_most_informative_features()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRlYg5PJEB32"
      },
      "source": [
        "# Serious testing\r\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\r\n",
        "\r\n",
        "def do_evaluation (pairs, pos_label='pos', verbose=True):\r\n",
        "    predicted, actual = zip(*pairs)\r\n",
        "    precision = precision_score(actual,predicted,pos_label=pos_label)\r\n",
        "    recall = recall_score(actual,predicted,pos_label=pos_label)\r\n",
        "    accuracy = accuracy_score(actual,predicted)\r\n",
        "    f1 = f1_score(actual,predicted,pos_label=pos_label)\r\n",
        "    if verbose:\r\n",
        "      print_results(precision, recall, accuracy, f1, pos_label)\r\n",
        "    return (precision, recall, accuracy, f1)\r\n",
        "\r\n",
        "def print_results (precision, recall, accuracy, f1, pos_label):\r\n",
        "    banner =  'Evaluation with pos label = %s' % pos_label\r\n",
        "    print()\r\n",
        "    print(banner)\r\n",
        "    print('=' * len(banner))\r\n",
        "    print('{0:10s} {1:.1f} %'.format('Precision',precision*100))\r\n",
        "    print('{0:10s} {1:.1f} %'.format('Recall',recall*100))\r\n",
        "    print('{0:10s} {1:.1f} %'.format('Accuracy',accuracy*100))\r\n",
        "    print('{0:10s} {1:.1f} %'.format('F1 score',f1*100))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIKSEvKUyJpv",
        "outputId": "dcd8d134-7923-4fd7-f6a4-d1ebe60cf327"
      },
      "source": [
        "pairs = [(classifier.classify(example), actual) for (example, actual) in test_set]\r\n",
        "print(pairs)\r\n",
        "print(pairs[0])\r\n",
        "print(pairs[99])\r\n",
        "print(pairs[100])\r\n",
        "print(pairs[199])\r\n",
        "print()\r\n",
        "i = 0\r\n",
        "while i < 100:\r\n",
        "  if pairs[i][0] == 'neg':\r\n",
        "    print(pairs[i])\r\n",
        "  i = i + 1\r\n",
        "\r\n",
        "# \r\n",
        "precision, recall, accuracy, f1 = do_evaluation (pairs, pos_label='pos')\r\n",
        "precision, recall, accuracy, f1 = do_evaluation (pairs, pos_label='neg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('neg', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('neg', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('neg', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('pos', 'pos'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('neg', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('pos', 'neg'), ('neg', 'neg')]\n",
            "('pos', 'pos')\n",
            "('pos', 'pos')\n",
            "('neg', 'neg')\n",
            "('neg', 'neg')\n",
            "\n",
            "('neg', 'pos')\n",
            "('neg', 'pos')\n",
            "('neg', 'pos')\n",
            "\n",
            "Evaluation with pos label = pos\n",
            "===============================\n",
            "Precision  64.7 %\n",
            "Recall     97.0 %\n",
            "Accuracy   72.0 %\n",
            "F1 score   77.6 %\n",
            "\n",
            "Evaluation with pos label = neg\n",
            "===============================\n",
            "Precision  94.0 %\n",
            "Recall     47.0 %\n",
            "Accuracy   72.0 %\n",
            "F1 score   62.7 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujSFljlNEeCx",
        "outputId": "b0a0e744-a2b0-4edd-b86b-67522da63ce1"
      },
      "source": [
        "pairs = [(classifier.classify(example), actual) for (example, actual) in test_set]\r\n",
        "#pairs = [(classifier.classify(example), actual) for (example, actual) in test_set]\r\n",
        "\r\n",
        "print(len(pairs[0]))\r\n",
        "print( pairs[0] )\r\n",
        "\r\n",
        "\r\n",
        "precision, recall, accuracy = do_evaluation (pairs, pos_label='pos')\r\n",
        "print(precision)\r\n",
        "print(recall)\r\n",
        "print(accuracy)\r\n",
        "precision, recall, accuracy = do_evaluation (pairs, pos_label='neg')\r\n",
        "print(precision)\r\n",
        "print(recall)\r\n",
        "print(accuracy)\r\n",
        "\r\n",
        "pos_guesses = [p for (p,a) in pairs if p=='pos']\r\n",
        "pos_actual = [a for (p,a) in pairs if a=='pos']\r\n",
        "do_evaluation (pairs, pos_label='neg')\r\n",
        "print('Note that {:.1%} of our classifier guesses were positive'.format(float(len(pos_guesses))/len(pairs)))\r\n",
        "print('While {:.1%} of the reviews were actually positive'.format(float(len(pos_actual))/len(pairs)))\r\n",
        "# to see the actual pairs that came out of the test uncomment the next line\r\n",
        "#pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "('pos', 'pos')\n",
            "pos\n",
            "\n",
            "Evaluation with pos label = pos\n",
            "===============================\n",
            "Precision  64.7\n",
            "Recall     97.0\n",
            "Accuracy   72.0\n",
            "0.6466666666666666\n",
            "0.97\n",
            "0.72\n",
            "\n",
            "Evaluation with pos label = neg\n",
            "===============================\n",
            "Precision  94.0\n",
            "Recall     47.0\n",
            "Accuracy   72.0\n",
            "0.94\n",
            "0.47\n",
            "0.72\n",
            "\n",
            "Evaluation with pos label = neg\n",
            "===============================\n",
            "Precision  94.0\n",
            "Recall     47.0\n",
            "Accuracy   72.0\n",
            "Note that 75.0% of our classifier guesses were positive\n",
            "While 50.0% of the reviews were actually positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sbD9pUlEyh_",
        "outputId": "6fd78091-5ea2-4126-f34d-089f9c180869"
      },
      "source": [
        "# SVM Classifier\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "\r\n",
        "import os.path\r\n",
        "\r\n",
        "def add_data_from_files (file_list,data_list):\r\n",
        "    for f in file_list:\r\n",
        "        with open(f,'r') as fh:\r\n",
        "            data_list.append(fh.read())\r\n",
        "\r\n",
        "home = os.getenv('HOME')\r\n",
        "# This is where MY NLTK data is.  Yours should be in a similar place relative\r\n",
        "# to what your machine thinks is HOME.\r\n",
        "data_dir = os.path.join(home,'nltk_data/corpora/movie_reviews/')\r\n",
        "\r\n",
        "clses = ['pos','neg']\r\n",
        "\r\n",
        "#  The data is in the data_dir, sorted into subdirectories, one for each class.\r\n",
        "data_dirs = [os.path.join(data_dir,cls) for cls in clses]\r\n",
        "#  We use a somewhat more traditional feature weights, called TFIDF weights\r\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\r\n",
        "\r\n",
        "# We're going to compute 4 lists training data and labels, test data a nd labels\r\n",
        "train_labels = []\r\n",
        "test_labels = []\r\n",
        "\r\n",
        "train_data = []\r\n",
        "test_data = []\r\n",
        "training_proportion = (9,10)\r\n",
        "for i,cls  in enumerate(clses):\r\n",
        "    d_dir = data_dirs[i]\r\n",
        "    os.chdir(d_dir)\r\n",
        "    cls_files = os.listdir(d_dir)\r\n",
        "    num_cls_files = len(cls_files)\r\n",
        "    training_index = int(training_proportion[0] *(num_cls_files/training_proportion[1]))\r\n",
        "    train_labels.extend(cls for f in cls_files[:training_index])\r\n",
        "    test_labels.extend(cls for f in cls_files[training_index:])\r\n",
        "    add_data_from_files (cls_files[:training_index],train_data)\r\n",
        "    add_data_from_files (cls_files[training_index:],test_data)\r\n",
        "\r\n",
        "# Now with data set represented as a list of strings (one from each file),\r\n",
        "# extract the TFIDF features\r\n",
        "train_features = vectorizer.fit_transform(train_data)\r\n",
        "\r\n",
        "#  We extract features from the test data using the same vectorizer\r\n",
        "#  trained on training data. The TFIDF feature model has been fit to\r\n",
        "#  (depends only on) the training data.\r\n",
        "test_features = vectorizer.transform(test_data)\r\n",
        "\r\n",
        "# Create an SVM classifier instance\r\n",
        "clf = LinearSVC(loss='squared_hinge', penalty=\"l2\", dual=False, tol=1e-3)\r\n",
        "\r\n",
        "# Train (or \"fit\") the model to the training data.\r\n",
        "clf.fit(train_features, train_labels)\r\n",
        "\r\n",
        "# Test the model on the test data.\r\n",
        "predicted_labels = clf.predict(test_features)\r\n",
        "\r\n",
        "# Evaluate the results\r\n",
        "pos_guesses = [p for p in predicted_labels if p=='pos']\r\n",
        "pos_actual = [p for p in test_labels if p=='pos']\r\n",
        "print('Note that {:.1%} of our classifier guesses were positive'.format(float(len(pos_guesses))/len(test_labels)))\r\n",
        "print('While {:.1%} of the reviews were actually positive'.format(float(len(pos_actual))/len(test_labels)))\r\n",
        "en = do_evaluation (zip(predicted_labels,test_labels), pos_label='pos', verbose=True)\r\n",
        "print(en)\r\n",
        "en = do_evaluation (zip(predicted_labels,test_labels), pos_label= 'neg', verbose=True)\r\n",
        "print(en)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note that 52.0% of our classifier guesses were positive\n",
            "While 50.0% of the reviews were actually positive\n",
            "\n",
            "Evaluation with pos label = pos\n",
            "===============================\n",
            "Precision  84.6\n",
            "Recall     88.0\n",
            "Accuracy   86.0\n",
            "(0.8461538461538461, 0.88, 0.86)\n",
            "\n",
            "Evaluation with pos label = neg\n",
            "===============================\n",
            "Precision  87.5\n",
            "Recall     84.0\n",
            "Accuracy   86.0\n",
            "(0.875, 0.84, 0.86)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
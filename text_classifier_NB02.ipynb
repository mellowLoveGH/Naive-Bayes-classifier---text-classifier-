{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classifier_NB02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG-aO0icit82"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download()\r\n",
        "# here to download movie_reviews and stopwords packages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLnVQQOCi8p3",
        "outputId": "9011a969-1667-4d06-dc0b-0a68b1aa332e"
      },
      "source": [
        "print('step 1: ')\r\n",
        "from nltk.corpus import movie_reviews as mr\r\n",
        "#print(mr.readme())\r\n",
        "# divide the movie_reviews into 2 categories: positive and negative\r\n",
        "positive = mr.fileids('pos')\r\n",
        "negative = mr.fileids('neg')\r\n",
        "# form a new movie_reviews as a dictionary by positive and negative\r\n",
        "new_mr = dict(pos = positive, neg = negative)\r\n",
        "# the length of positive and negative lists are both 1000\r\n",
        "posLen = len(new_mr['pos'])\r\n",
        "negLen = len(new_mr['neg'])\r\n",
        "print('movie reviews are loaded with ')\r\n",
        "print('%s positive reviews ' % posLen)\r\n",
        "print('%s negative reviews ' % negLen)\r\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 1: \n",
            "movie reviews are loaded with \n",
            "1000 positive reviews \n",
            "1000 negative reviews \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pHo2d6XjL98",
        "outputId": "b079582e-64b4-4084-e9a5-f62b23051430"
      },
      "source": [
        "print('step 2: ')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "sw = stopwords.words('english');\r\n",
        "# here in the stopwords list 'sw'\r\n",
        "# it should remove some words from the list\r\n",
        "# such as: 'no','nor','not','too', and\r\n",
        "# \"don't\",\"aren't\", \"couldn't\", 'didn', \"didn't\", \r\n",
        "# \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\",\r\n",
        "#  \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\",\r\n",
        "#  \"won't\", \"wouldn't\"\r\n",
        "sw.remove('no')\r\n",
        "sw.remove('nor')\r\n",
        "sw.remove('not')\r\n",
        "sw.remove('too')\r\n",
        "sw1 = []\r\n",
        "for v in sw:\r\n",
        "    if not v.endswith(\"n't\"):\r\n",
        "        sw1.append(v)\r\n",
        "    #else:\r\n",
        "        #print(v)\r\n",
        "sw1.remove('ain')\r\n",
        "sw1.remove('aren')\r\n",
        "sw1.remove('couldn')\r\n",
        "sw1.remove('didn')\r\n",
        "sw1.remove('doesn')\r\n",
        "sw1.remove('hadn')\r\n",
        "sw1.remove('haven')\r\n",
        "sw1.remove('isn')\r\n",
        "sw1.remove('mightn')\r\n",
        "sw1.remove('hasn')\r\n",
        "sw1.remove('mustn')\r\n",
        "sw1.remove('needn')\r\n",
        "sw1.remove('shan')\r\n",
        "sw1.remove('wasn')\r\n",
        "sw1.remove('weren')\r\n",
        "sw1.remove('wouldn')\r\n",
        "sw = sw1\r\n",
        "#print(len(sw))\r\n",
        "print('there are %s stopwords loaded' % len(sw1))\r\n",
        "\r\n",
        "# input a word, \r\n",
        "# determine whether it is a stopword or not\r\n",
        "# remove all punctuations from the word\r\n",
        "def pre_process(word):\r\n",
        "  # remove punctuations except apostrophe\r\n",
        "  w = \"\"\r\n",
        "  hasAlphabet = False\r\n",
        "  for i in word:\r\n",
        "    if i >= 'a' and i <= 'z':\r\n",
        "      w = w + i\r\n",
        "      hasAlphabet = True\r\n",
        "    elif i >= 'A' and i <= 'Z':\r\n",
        "      w = w + i\r\n",
        "      hasAlphabet = True\r\n",
        "    elif i == '\\'': # apostrophe\r\n",
        "      w = w + i\r\n",
        "  #print(w)\r\n",
        "  # check whether it is stopword\r\n",
        "  if hasAlphabet and w not in sw:\r\n",
        "    return w.lower()\r\n",
        "  return ''\r\n",
        "\r\n",
        "# selection the 3000 most important words.\r\n",
        "# find the 3000 most important words by frequency\r\n",
        "# top, is how many words in the list to be returned\r\n",
        "def importantWords(corpus, file, top):\r\n",
        "  iw = []\r\n",
        "  words_dic = {}\r\n",
        "  pLen = len(file['pos'])\r\n",
        "  i = 0\r\n",
        "  while i < pLen:\r\n",
        "    movie_review = corpus.words(file['pos'][i])\r\n",
        "    for w in movie_review:\r\n",
        "      w = pre_process(w)\r\n",
        "      if w not in words_dic:\r\n",
        "        words_dic[w] = 1\r\n",
        "      else:\r\n",
        "        words_dic[w] = words_dic[w] + 1\r\n",
        "    i = i + 1\r\n",
        "  nLen = len(file['neg'])\r\n",
        "  i = 0\r\n",
        "  while i < nLen:\r\n",
        "    movie_review = corpus.words(file['neg'][i])\r\n",
        "    for w in movie_review:\r\n",
        "      w = pre_process(w)\r\n",
        "      if len(w) > 0:\r\n",
        "        if w not in words_dic:\r\n",
        "          words_dic[w] = 1\r\n",
        "        else:\r\n",
        "          words_dic[w] = words_dic[w] + 1\r\n",
        "      else:\r\n",
        "        continue\r\n",
        "    i = i + 1\r\n",
        "  words_dic.pop('') # remove space\r\n",
        "  sorted_words_dic = sorted(words_dic.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)\r\n",
        "  # add the top words to the list to be returned\r\n",
        "  i = 0\r\n",
        "  while i < top:\r\n",
        "    iw.append(sorted_words_dic[i])\r\n",
        "    i = i + 1\r\n",
        "  return iw\r\n",
        "\r\n",
        "# the input parameter is a list consisting of words\r\n",
        "# and return a dictionary which includes useful words after pre-processing\r\n",
        "def feature_selection(words):\r\n",
        "  wd = {}\r\n",
        "  for word in words:\r\n",
        "    word = pre_process(word)\r\n",
        "    # if len(word) > 0 and word in iw and word not in wd:\r\n",
        "    if len(word) > 0 and word not in wd:\r\n",
        "      wd[word] = True\r\n",
        "  return wd\r\n",
        "\r\n",
        "# collect several/some/many dict-type positive or negative movie reviews into a list\r\n",
        "# for training or testing\r\n",
        "# the returned list is composed of tuple\r\n",
        "# every tuple is the ict-type positive or negative movie reviews + clf\r\n",
        "# corpus, should be the 'mr'\r\n",
        "# files, should be the 'new_mr'\r\n",
        "# clf, is the classification: pos or neg\r\n",
        "def collect_features(corpus, files, startID, endID, clf):\r\n",
        "  col = []\r\n",
        "  i = startID\r\n",
        "  while i < endID:\r\n",
        "    dic = feature_selection(corpus.words(files[clf][i]))\r\n",
        "    tup = (dic, clf)\r\n",
        "    col.append(tup)\r\n",
        "    i = i + 1\r\n",
        "  return col\r\n",
        "\r\n",
        "#top = 3000\r\n",
        "#iw = importantWords(mr, new_mr, top)\r\n",
        "#print('selection the 3000 most important words')\r\n",
        "\r\n",
        "\r\n",
        "train_start_id = 100\r\n",
        "train_end_id = 900\r\n",
        "test_start_id = 900\r\n",
        "test_end_id = 1000\r\n",
        "\r\n",
        "pos_train = collect_features(mr, new_mr, train_start_id, train_end_id, 'pos')\r\n",
        "pos_test = collect_features(mr, new_mr, test_start_id, test_end_id, 'pos')\r\n",
        "\r\n",
        "neg_train = collect_features(mr, new_mr, train_start_id, train_end_id, 'neg')\r\n",
        "neg_test = collect_features(mr, new_mr, test_start_id, test_end_id, 'neg')\r\n",
        "\r\n",
        "# whole sets\r\n",
        "train_set = pos_train + neg_train\r\n",
        "test_set = pos_test + neg_test\r\n",
        "\r\n",
        "print('pre-processing and divide the movie reviews into training and testing set')\r\n",
        "print('training set: 90% \\ntesting set: 10%')\r\n",
        "\"\"\"\r\n",
        "print(len(neg_training))\r\n",
        "print(len(neg_test))\r\n",
        "print(len(pos_training))\r\n",
        "print(len(pos_test))\r\n",
        "print(len(train_set))\r\n",
        "print(len(test_set))\r\n",
        "\"\"\"\r\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 2: \n",
            "there are 141 stopwords loaded\n",
            "pre-processing and divide the movie reviews into training and testing set\n",
            "training set: 90% \n",
            "testing set: 10%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R849hFlOkose",
        "outputId": "b7bcd38b-a48f-448a-b1b1-4edc327aa79b"
      },
      "source": [
        "print('step 3:')\r\n",
        "# text classification\r\n",
        "# Use a Naive Bayes Classifier\r\n",
        "from nltk.classify import NaiveBayesClassifier\r\n",
        "print('text classification: Naive Bayes Text classification')\r\n",
        "\r\n",
        "# Train a classifier on our training data.\r\n",
        "classifier = NaiveBayesClassifier.train(train_set)\r\n",
        "print('using NB classifier to train the model on 90% of movie reviews')\r\n",
        "\r\n",
        "# get a certain moview review\r\n",
        "# corpus, 'rm'\r\n",
        "# files, 'new_rm'\r\n",
        "# clf, 'pos' or 'neg'\r\n",
        "# fileID, n-th\r\n",
        "# startID, endID, the sub-string of the retrieved moview review\r\n",
        "def get_mr_text(corpus, files, clf, fileID, startID=0, endID=None):\r\n",
        "  words = list(corpus.words(files[clf][fileID]))\r\n",
        "  return ' '.join(words[startID:endID])\r\n",
        "\r\n",
        "#print(get_mr_text(mr, new_mr, 'pos', 900) )\r\n",
        "#print(get_mr_text(mr, new_mr, 'neg', 900) )\r\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 3:\n",
            "text classification: Naive Bayes Text classification\n",
            "using NB classifier to train the model on 90% of movie reviews\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L38GAmEikvP9",
        "outputId": "6e1c207d-31b2-40db-e6fa-9a3e7b151282"
      },
      "source": [
        "print('step 4:')\r\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\r\n",
        "print('evaluate trained model on 10% of movie reviews')\r\n",
        "\r\n",
        "# pairs, after using the model to test, it will generate every testing case as a tuple\r\n",
        "# such as: ( 'pos', 'pos' ), the first one is predicted one, and the latter one is actual one\r\n",
        "# pos_label, 'pos' or 'neg'\r\n",
        "# printout, True means to print it out and False means not\r\n",
        "def evaluate(pairs, pos_label, printout):\r\n",
        "    predicted, actual = zip(*pairs)\r\n",
        "    precision = precision_score(actual,predicted,pos_label=pos_label)\r\n",
        "    recall = recall_score(actual,predicted,pos_label=pos_label)\r\n",
        "    accuracy = accuracy_score(actual,predicted)\r\n",
        "    f1 = f1_score(actual,predicted,pos_label=pos_label)\r\n",
        "    if printout:\r\n",
        "      print_evaluation(precision, recall, accuracy, f1, pos_label)\r\n",
        "    return (precision, recall, accuracy, f1)\r\n",
        "\r\n",
        "def print_evaluation(precision, recall, accuracy, f1, pos_label):\r\n",
        "    title =  'Evaluation with pos label = %s' % pos_label\r\n",
        "    print(title)\r\n",
        "    print('=' * len(title))\r\n",
        "    print('{0:10s} {1:.2f} %'.format('Precision',precision*100))\r\n",
        "    print('{0:10s} {1:.2f} %'.format('Recall',recall*100))\r\n",
        "    print('{0:10s} {1:.2f} %'.format('Accuracy',accuracy*100))\r\n",
        "    print('{0:10s} {1:.2f} %'.format('F1 score',f1*100))\r\n",
        "\r\n",
        "pairs = [(classifier.classify(predicted), actual) for (predicted, actual) in test_set]\r\n",
        "printout = True\r\n",
        "precision, recall, accuracy, f1 = evaluate(pairs, 'pos', printout)\r\n",
        "print()\r\n",
        "precision, recall, accuracy, f1 = evaluate(pairs, 'neg', printout)\r\n",
        "print()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 4:\n",
            "evaluate trained model on 10% of movie reviews\n",
            "Evaluation with pos label = pos\n",
            "===============================\n",
            "Precision  63.40 %\n",
            "Recall     97.00 %\n",
            "Accuracy   70.50 %\n",
            "F1 score   76.68 %\n",
            "\n",
            "Evaluation with pos label = neg\n",
            "===============================\n",
            "Precision  93.62 %\n",
            "Recall     44.00 %\n",
            "Accuracy   70.50 %\n",
            "F1 score   59.86 %\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0TfrRJTmEbH",
        "outputId": "f1b87c2e-6bbf-488e-841c-a63a7c9dba0d"
      },
      "source": [
        "print('step 5:')\r\n",
        "print('testing the exclusive movie reviews with this model')\r\n",
        "# new testing examples excluded from the move_reviews set\r\n",
        "movie_review = 'Inception is the best movie ever'\r\n",
        "prediction = classifier.classify(feature_selection(movie_review.split()))\r\n",
        "print(prediction)\r\n",
        "movie_review = \"I don't know how anyone could sit through Inception\"\r\n",
        "prediction = classifier.classify(feature_selection(movie_review.split()))\r\n",
        "print(prediction)\r\n",
        "classifier.show_most_informative_features()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 5:\n",
            "testing the exclusive movie reviews with this model\n",
            "neg\n",
            "neg\n",
            "Most Informative Features\n",
            "               ludicrous = True              neg : pos    =     21.0 : 1.0\n",
            "                 idiotic = True              neg : pos    =     16.3 : 1.0\n",
            "             outstanding = True              pos : neg    =     15.0 : 1.0\n",
            "                 chuckle = True              neg : pos    =     14.3 : 1.0\n",
            "                  avoids = True              pos : neg    =     11.0 : 1.0\n",
            "              astounding = True              pos : neg    =     11.0 : 1.0\n",
            "                poignant = True              pos : neg    =     10.6 : 1.0\n",
            "                 offbeat = True              pos : neg    =     10.3 : 1.0\n",
            "             fascination = True              pos : neg    =     10.3 : 1.0\n",
            "                religion = True              pos : neg    =      9.8 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5Zc7BIywAPo",
        "outputId": "3c256e01-bdbd-4c40-d31e-e60c5aed7d28"
      },
      "source": [
        "def unigram_features (words):\r\n",
        "    \"\"\"\r\n",
        "    This is the simplest possible feature representation of a document.\r\n",
        "\r\n",
        "    Each word is a feature.\r\n",
        "    \"\"\"\r\n",
        "    return dict((word, True) for word in words)\r\n",
        "\r\n",
        "# the input parameter is a list consisting of words\r\n",
        "# and return a dictionary which includes useful words after pre-processing\r\n",
        "def feature_selection1(words):\r\n",
        "  wd = {}\r\n",
        "  for word in words:\r\n",
        "    word = pre_process(word)\r\n",
        "    # if len(word) > 0 and word in iw and word not in wd:\r\n",
        "    if len(word) > 0:\r\n",
        "      wd[word] = True\r\n",
        "  return wd\r\n",
        "\r\n",
        "def focusWords(word):\r\n",
        "  adj_list = ['ing', 'ous', 'ic', 'ed', 'ive', 'ble', 'al', 'ant', 'ary', 'ful', 'less', 'y']\r\n",
        "  for w in adj_list:\r\n",
        "      if word.endswith(w):\r\n",
        "          return True\r\n",
        "  neg_list = ['no', 'nor', 'not', \"too\", 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'haven', 'isn', 'mightn', 'hasn', 'mustn', 'needn', 'shan', 'wasn', 'weren', 'wouldn']\r\n",
        "  for w in neg_list:\r\n",
        "      if word == w:\r\n",
        "          return True\r\n",
        "  if word.endswith(\"n't\"):\r\n",
        "      return True\r\n",
        "  return False\r\n",
        "def feature_selection_update(words):\r\n",
        "  wd = {}\r\n",
        "  for word in words:\r\n",
        "    word = pre_process(word)\r\n",
        "    if focusWords(word):\r\n",
        "      wd[word] = True\r\n",
        "  return wd\r\n",
        "\r\n",
        "tmp_list = [ 'I', 'Love', 'you', 'I', 'Love', 'you', 'this', 'is', 'not', 'exciting', 'wouldnt',\"wouldn't\" ]\r\n",
        "print(tmp_list)\r\n",
        "print(unigram_features(tmp_list))\r\n",
        "print(feature_selection1(tmp_list))\r\n",
        "print(feature_selection_update(tmp_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'Love', 'you', 'I', 'Love', 'you', 'this', 'is', 'not', 'exciting', 'wouldnt', \"wouldn't\"]\n",
            "{'I': True, 'Love': True, 'you': True, 'this': True, 'is': True, 'not': True, 'exciting': True, 'wouldnt': True, \"wouldn't\": True}\n",
            "{'i': True, 'love': True, 'not': True, 'exciting': True, 'wouldnt': True, \"wouldn't\": True}\n",
            "{'not': True, 'exciting': True, \"wouldn't\": True}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}